# -*- coding: utf-8 -*-
"""TensorflowCoursework.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HP-ceOE8Im0O8NzSIXROJvwQAw2vmSfP

# High Perfromance Computing and Big Data - Tensorflow Task (50%)

### Dylan Pace - 16026660
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from tensorboard.plugins.hparams import api as hp
print(tf.__version__)
#Import the necessary libararies to make tensorflow and python function.

img_path = 'Caltech-256/Caltech-256'
#Import the image path do the model can use the images as input.
input_shape = (1, 128, 128, 3)
#Define the input shape of the image so they may all be the same dimensions. 
batch_size = 32
#The batch size is how many images will be used during each pass. 

#Create an augmented data generator.
train_data_gen = tf.keras.preprocessing.image.ImageDataGenerator(
    rescale=1./255,
    shear_range=10,
    zoom_range=0.2,
    horizontal_flip=True,
    validation_split=0.1)

#Create a non-augmented data generator.
valid_data_gen = tf.keras.preprocessing.image.ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2)

#Create a generator to load images using the augmented data generator train_data_gen.
#Set the seed to 1.
train_gen = train_data_gen.flow_from_directory(img_path, subset='training',
        color_mode='rgb', target_size=input_shape[1:3],
        batch_size=batch_size, class_mode='categorical',seed=1)

#Create a generator to load images using the non-augmented data generator valid_data_gen.
#Using the same seed means that the random split of validation images with be the same
#for both augmented and non-augmented generators.
valid_gen = valid_data_gen.flow_from_directory(img_path, subset='validation',
        color_mode='rgb', target_size=input_shape[1:3],
        batch_size=batch_size, class_mode='categorical',seed=1)

train_imgs, train_labels = next(train_gen)
valid_imgs, valid_labels = next(valid_gen)

"""## Plain Convolutional Network"""

#Network will be fully connected and run a large number of different layers.
inputs=tf.keras.layers.Input(shape=input_shape[1:])
#Input shape has the same dimensions as the images.
layer1=tf.keras.layers.ZeroPadding2D(padding=(1, 1), data_format=None)(inputs)
#Convolutional layers for more accurate image recognition.
layer2=tf.keras.layers.Conv2D(kernel_size=3,filters=10)(layer1)
layer3=tf.keras.layers.BatchNormalization()(layer2)
#Batch normalisation helps normalise the mean.
layer4=tf.keras.layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0)(layer3)
layer5=tf.keras.layers.DepthwiseConv2D(3, strides=(1, 1), padding='valid')(layer4)
layer6=tf.keras.layers.BatchNormalization()(layer5)
layer7=tf.keras.layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0)(layer6)
layer8=tf.keras.layers.Conv2D(kernel_size=(3,3),filters=20, activation='relu')(layer7)
layer9=tf.keras.layers.BatchNormalization()(layer8)
layer10=tf.keras.layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0)(layer9)
layer11=tf.keras.layers.ZeroPadding2D(padding=(1, 1), data_format=None)(layer10)
layer12=tf.keras.layers.DepthwiseConv2D(3, strides=(1, 1), padding='valid')(layer11)
layer13=tf.keras.layers.BatchNormalization()(layer12)
layer14=tf.keras.layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0)(layer13)
layer15=tf.keras.layers.Conv2D(kernel_size=(3,3),filters=40, activation='relu')(layer14)
layer16=tf.keras.layers.BatchNormalization()(layer15)
layer17=tf.keras.layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0)(layer16)
layer18=tf.keras.layers.DepthwiseConv2D(3, strides=(1, 1), padding='valid')(layer17)
layer19=tf.keras.layers.BatchNormalization()(layer18)
layer20=tf.keras.layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0)(layer19)
layer21=tf.keras.layers.Conv2D(kernel_size=(3,3),filters=80, activation='relu')(layer20)
layer22=tf.keras.layers.BatchNormalization()(layer21)
layer23=tf.keras.layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0)(layer22)
layer24=tf.keras.layers.ZeroPadding2D(padding=(1, 1), data_format=None)(layer23)
layer25=tf.keras.layers.DepthwiseConv2D(3, strides=(1, 1), padding='valid')(layer24)
layer26=tf.keras.layers.BatchNormalization()(layer25)
layer27=tf.keras.layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0)(layer26)
layer28=tf.keras.layers.Conv2D(kernel_size=(3,3),filters=100, activation='relu')(layer27)
layer29=tf.keras.layers.BatchNormalization()(layer28)
layer30=tf.keras.layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0)(layer29)
layer31=tf.keras.layers.DepthwiseConv2D(3, strides=(1, 1), padding='valid')(layer30)
layer32=tf.keras.layers.BatchNormalization()(layer31)
layer33=tf.keras.layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0)(layer32)
layer34=tf.keras.layers.Conv2D(kernel_size=(3,3),filters=100, activation='relu')(layer33)
layer35=tf.keras.layers.BatchNormalization()(layer34)
layer36=tf.keras.layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0)(layer35)
layer37=tf.keras.layers.ZeroPadding2D(padding=(1, 1), data_format=None)(layer36)
layer38=tf.keras.layers.DepthwiseConv2D(3, strides=(1, 1), padding='valid')(layer37)
layer39=tf.keras.layers.BatchNormalization()(layer38)
layer40=tf.keras.layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0)(layer39)
layer41=tf.keras.layers.Conv2D(kernel_size=(3,3),filters=100, activation='relu')(layer40)
layer42=tf.keras.layers.BatchNormalization()(layer41)
layer43=tf.keras.layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0)(layer42)
layer44=tf.keras.layers.DepthwiseConv2D(3, strides=(1, 1), padding='valid')(layer43)
layer45=tf.keras.layers.BatchNormalization()(layer44)
layer46=tf.keras.layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0)(layer45)
layer47=tf.keras.layers.Conv2D(kernel_size=(3,3),filters=100, activation='relu')(layer46)
layer48=tf.keras.layers.BatchNormalization()(layer47)
layer49=tf.keras.layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0)(layer48)
layer50=tf.keras.layers.DepthwiseConv2D(3, strides=(1, 1), padding='valid')(layer49)
layer51=tf.keras.layers.BatchNormalization()(layer50)
layer52=tf.keras.layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0)(layer51)
layer53=tf.keras.layers.Conv2D(kernel_size=(3,3),filters=100, activation='relu')(layer52)
layer54=tf.keras.layers.BatchNormalization()(layer53)
layer55=tf.keras.layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0)(layer54)
layer56=tf.keras.layers.DepthwiseConv2D(3, strides=(1, 1), padding='valid')(layer55)
layer57=tf.keras.layers.BatchNormalization()(layer56)
layer58=tf.keras.layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0)(layer57)
layer59=tf.keras.layers.Conv2D(kernel_size=(3,3),filters=100, activation='relu')(layer58)
layer60=tf.keras.layers.BatchNormalization()(layer59)
layer61=tf.keras.layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0)(layer60)
layer62=tf.keras.layers.DepthwiseConv2D(3, strides=(1, 1), padding='valid')(layer61)
layer63=tf.keras.layers.BatchNormalization()(layer62)
layer64=tf.keras.layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0)(layer63)
layer65=tf.keras.layers.Conv2D(kernel_size=(3,3),filters=100, activation='relu')(layer64)
layer66=tf.keras.layers.BatchNormalization()(layer65)
layer67=tf.keras.layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0)(layer66)
layer68=tf.keras.layers.DepthwiseConv2D(3, strides=(1, 1), padding='valid')(layer67)
layer69=tf.keras.layers.BatchNormalization()(layer68)
layer70=tf.keras.layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0)(layer69)
layer71=tf.keras.layers.Conv2D(kernel_size=(3,3),filters=100, activation='relu')(layer70)
layer72=tf.keras.layers.BatchNormalization()(layer71)
layer73=tf.keras.layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0)(layer72)
layer74=tf.keras.layers.ZeroPadding2D(padding=(1, 1), data_format=None)(layer73)
layer75=tf.keras.layers.DepthwiseConv2D(3, strides=(1, 1), padding='valid')(layer74)
layer76=tf.keras.layers.BatchNormalization()(layer75)
layer77=tf.keras.layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0)(layer76)
#Dropout will helps to stop the least accurate 20% of neuron in the model to ensure accuracy.
layer78=tf.keras.layers.Dropout(0.2)(layer77)
#Flatten the network down to get a final result.
layer79=tf.keras.layers.Flatten()(layer78)
layer80=tf.keras.layers.Dense(10)(layer79)
layer81=tf.keras.layers.Dense(257)(layer80)
outputs=tf.keras.layers.Softmax()(layer81)

#Save the model in a variable.
model=tf.keras.Model(inputs,outputs)
#Create a summary of the network structure.
model.summary()

#Compile the model.
model.compile(loss='categorical_crossentropy',
                optimizer=tf.keras.optimizers.Adam(0.001),
                metrics=['accuracy'])

#Connect to tensorboard.
tb = tf.keras.callbacks.TensorBoard(log_dir='logs/cnn_transfer8', write_graph=True)
tb.set_model(model)

"""## Default Deep Convolutional Network"""

filters = 8 #32
#Filters are used for convolutional layers.
lr = .0001 #.001
#The learning rate of the model.
epochs = 80
#The epochs are how many times the model will do a full run.
batch_size = 32
decay = lr/epochs
img_size =  128
layer = 3

tf.keras.backend.clear_session()

#Start of the model.
model = tf.keras.Sequential()
#Adding convolutional layers will helped with image recognition.
model.add(tf.keras.layers.Conv2D(filters = (filters),
                                 kernel_size= (3,3),
                                 input_shape = (img_size,img_size,3),
                                 padding = 'same'))
model.add(tf.keras.layers.BatchNormalization())
#Batch normalisation will automatically standarise inputs to the layer. 
model.add(tf.keras.layers.Activation('relu'))
#The relu function is a non-linear function to help with backpropogation. 
model.add(tf.keras.layers.MaxPool2D(pool_size = (2,2)))
#Max pooling can determine the maxmimum features being used by the model. 

#Loop through the important functions a few times to build the model.
for i in range(layer):
    filters1 = filters * (2**(i+1))
    model.add(tf.keras.layers.Conv2D(filters = filters1, kernel_size= (3,3), padding = 'same'))
    model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.Activation('relu'))
    model.add(tf.keras.layers.MaxPool2D(pool_size = (2,2)))

model.add(tf.keras.layers.Dropout(0.2))
#Flatten the dimensions of the tensor object into one. 
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(512))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Activation('relu')) 

#The final layer will give an output.
outputs = model.add(tf.keras.layers.Dense(257,activation='softmax'))
#The model can be compiled using optimisers, the learning rate, a loss functions and accuracy metrics. 
model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=lr,decay=decay), 
              loss='categorical_crossentropy', 
              metrics=['accuracy'])

#Shows a summary of the model created.
model.summary()

#Sets up a tensorboard to display the training and testing of the model.
tb = tf.keras.callbacks.TensorBoard(log_dir='logs\cnn_transfer11', write_graph=True)
tb.set_model(model)

#The model can be run using the fit_generator and the images will be trained and tested to increase
#the accuracy score. 
model.fit_generator(
        train_gen,
        shuffle=True,
        steps_per_epoch=6000/32,
        epochs=30,
        validation_data=valid_gen,
        validation_steps=1000/32)
#It uses both data generators and shuffles the images before each epoch so data input is always unpredictable.

epochs = 30
#Another form of data training and testing, this method can be connected to tensorboard to track the accuracy
#progress but seems to be less efficient in raising accuracy than the fit generator.
for epoch in range(0, valid_gen.samples * epochs, batch_size):
    #Load the training data.
    train_imgs, train_labels = next(train_gen)
    #Trains the data using each batch.
    train_CE, train_acc = model.train_on_batch(train_imgs, train_labels)

    #Load the validation data.
    valid_imgs, valid_labels = next(valid_gen)
    #Evaluates the images.
    valid_CE, valid_acc = model.evaluate(valid_imgs, valid_labels, verbose=0)

    #Prints the accuracy outputs.
    print('@epoch:', epoch / batch_size, 'train CE:', "{:3.2f}".format(train_CE), 'train acc:', "{:3.2f}".format(train_acc*100)
          , 'valid CE:', "{:3.2f}".format(valid_CE), 'valid acc:', "{:3.2f}".format(valid_acc*100))

    #Sends the data to tensorboard to track progress.
    tb.on_epoch_end(epoch, {'train_CE': train_CE, 'train_acc': train_acc * 100, 'valid_CE': valid_CE, 'valid_acc': valid_acc * 100})

tb.on_train_end(None)

"""## Parameter-Tuned Deep Convolutional Network"""

filters = 32
#Filters are used for convolutional layers.
lr = .001
#The learning rate of the model.
epochs = 80
#The epochs are how many times the model will do a full run.
batch_size = 32
decay = lr/epochs
img_size =  128
layer = 3

tf.keras.backend.clear_session()

#Start of the model.
model = tf.keras.Sequential()
#Adding convolutional layers will helped with image recognition.
model.add(tf.keras.layers.Conv2D(filters = (filters),
                                 kernel_size= (3,3),
                                 input_shape = (img_size,img_size,3),
                                 padding = 'same'))
model.add(tf.keras.layers.BatchNormalization())
#Batch normalisation will automatically standarise inputs to the layer. 
model.add(tf.keras.layers.Activation('relu'))
#The relu function is a non-linear function to help with backpropogation. 
model.add(tf.keras.layers.MaxPool2D(pool_size = (2,2)))
#Max pooling can determine the maxmimum features being used by the model. 

#Loop through the important functions a few times to build the model.
for i in range(layer):
    filters1 = filters * (2**(i+1))
    model.add(tf.keras.layers.Conv2D(filters = filters1, kernel_size= (3,3), padding = 'same'))
    model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.Activation('relu'))
    model.add(tf.keras.layers.MaxPool2D(pool_size = (2,2)))

model.add(tf.keras.layers.Dropout(0.2))
#Flatten the dimensions of the tensor object into one. 
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(512))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Activation('relu')) 

#The final layer will give an output.
outputs = model.add(tf.keras.layers.Dense(257,activation='softmax'))
#The model can be compiled using optimisers, the learning rate, a loss functions and accuracy metrics. 
model.compile(optimizer=tf.keras.optimizers.Adam(lr=lr,decay=decay), 
              loss='categorical_crossentropy', 
              metrics=['accuracy'])

#Shows a summary of the model created.
model.summary()

experiment_name = 'logs\cnn_experiment1'
#Sets up a tensorboard to display the training and testing of the model.
tb = tf.keras.callbacks.TensorBoard(log_dir='logs\cnn_transfer10', write_graph=True)
cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=experiment_name + '.h5py', monitor='val_loss', save_best_only=True, mode='min') 
tb.set_model(model)

#This model should get around 60% training accuracy and 50% testing accuracy.

#The model can be run using the fit_generator and the images will be trained and tested to increase
#the accuracy score. 
model.fit_generator(
        train_gen,
        shuffle=True,
        steps_per_epoch=6000/32,
        epochs=30,
        validation_data=valid_gen,
        validation_steps=1000/32)
#It uses both data generators and shuffles the images before each epoch so data input is always unpredictable.

epochs = 30
#Another form of data training and testing, this method can be connected to tensorboard to track the accuracy
#progress but seems to be less efficient in raising accuracy than the fit generator.
for epoch in range(0, valid_gen.samples * epochs, batch_size):
    #Load the training data.
    train_imgs, train_labels = next(train_gen)
    #Trains the data using each batch.
    train_CE, train_acc = model.train_on_batch(train_imgs, train_labels)

    #Load the validation data.
    valid_imgs, valid_labels = next(valid_gen)
    #Evaluates the images.
    valid_CE, valid_acc = model.evaluate(valid_imgs, valid_labels, verbose=0)

    #Prints the accuracy outputs.
    print('@epoch:', epoch / batch_size, 'train CE:', "{:3.2f}".format(train_CE), 'train acc:', "{:3.2f}".format(train_acc*100)
          , 'valid CE:', "{:3.2f}".format(valid_CE), 'valid acc:', "{:3.2f}".format(valid_acc*100))

    #Sends the data to tensorboard to track progress.
    tb.on_epoch_end(epoch, {'train_CE': train_CE, 'train_acc': train_acc * 100, 'valid_CE': valid_CE, 'valid_acc': valid_acc * 100})

tb.on_train_end(None)

class_names = list(valid_gen.class_indices)
#Gets the names of the different classes as it will use the model to try and correctly predict what
#the images are.

#Load a validation image.
X_img, X_labels = next(valid_gen)
#Use the model to predict what the image is of.
X_pred = model.predict(X_img)
#Evaluate the image.
test_CE, test_acc = model.evaluate(X_img, X_labels)
#Prints the accuracy score.
print('testing accuracy:', "{:3.2f}".format(test_acc*100))

#Loops through a selection of images and will try an predict what they are.
for i in range(9):
    plt.subplot(3, 3, i + 1, frameon=False)
    plt.axis('off')
    #Displays the images.
    plt.imshow(np.reshape(X_img[i], input_shape[1:]))
    #Predicts their title, what the model thinks the image is.
    plt.title(class_names[np.argmax(X_pred[i])])
plt.show(block=False)

"""## Residual Convolutional Network"""

#Define a method for s set of layers.
def ResidualBlock(inputs, filters):
    #Sets up convolutional layers.
    skip = tf.keras.layers.Conv2D(filters=filters,
                                kernel_size=(1,1))(inputs)
    conv = tf.keras.layers.Conv2D(filters=filters,
                                    kernel_size=(3,3),
                                    padding='SAME')(inputs)
    #Applies the activation layers.
    relu = tf.keras.layers.ReLU()(conv)
    ident = tf.keras.layers.Add()([skip, relu])
    skip_1 = ident
    conv_1 = tf.keras.layers.Conv2D(filters=filters,
                                    kernel_size=(3,3),
                                    padding='SAME')(ident)
    relu_1 = tf.keras.layers.ReLU()(conv_1)
    ident_1 = tf.keras.layers.Add()([skip_1, relu_1])
    #Pools the method layers.
    pool = tf.keras.layers.AveragePooling2D(pool_size=(2, 2),
                                              strides=(2, 2),
                                              padding='SAME')(ident_1)
    return pool

#Takes the image dimensions as input.
inputs = tf.keras.layers.Input(shape=input_shape[1:])
conv_1 = tf.keras.layers.Conv2D(filters=64,
                                kernel_size=(3,3),
                                padding='SAME')(inputs)
relu_1 = tf.keras.layers.ReLU()(conv_1)
pool_1 = tf.keras.layers.AveragePooling2D(pool_size=(2, 2),
                                          strides=(2, 2),
                                          padding='SAME')(relu_1)
#Calls upon the residual method.
pool_2 = ResidualBlock(pool_1, 128)
pool_3 = ResidualBlock(pool_2, 128)
bn = tf.keras.layers.BatchNormalization()(pool_3)

#Flattens the dimensions.
flattened = tf.keras.layers.Flatten()(bn)
dense = tf.keras.layers.Dense(257)(flattened)
outputs = tf.keras.layers.Softmax()(dense)

#Saves the model into a single variable.
convnet = tf.keras.Model(inputs, outputs)
#Creates a summary of the model to show the structure.
convnet.summary()

#Compiles the model in order for it to run.
convnet.compile(loss='categorical_crossentropy',
                optimizer=tf.keras.optimizers.Adam(0.0001),
                metrics=['accuracy'])

#Connects to tensorboard.
tb = tf.keras.callbacks.TensorBoard(log_dir='logs/deep_res_net_2', histogram_freq=0, batch_size=batch_size, write_graph=True, write_grads=False)
tb.set_model(convnet)

#The model can be run using the fit_generator and the images will be trained and tested to increase
#the accuracy score. 
convnet.fit_generator(
        train_gen,
        shuffle=True,
        steps_per_epoch=6000/32,
        epochs=30,
        validation_data=valid_gen,
        validation_steps=1000/32)
#It uses both data generators and shuffles the images before each epoch so data input is always unpredictable.

epochs = 30
#Another form of data training and testing, this method can be connected to tensorboard to track the accuracy
#progress but seems to be less efficient in raising accuracy than the fit generator.
for epoch in range(0, valid_gen.samples * epochs, batch_size):
    #Load the training data.
    train_imgs, train_labels = next(train_gen)
    #Trains the data using each batch.
    train_CE, train_acc = convnet.train_on_batch(train_imgs, train_labels)

    #Load the validation data.
    valid_imgs, valid_labels = next(valid_gen)
    #Evaluates the images.
    valid_CE, valid_acc = convnet.evaluate(valid_imgs, valid_labels, verbose=0)

    #Prints the accuracy outputs.
    print('@epoch:', epoch / batch_size, 'train CE:', "{:3.2f}".format(train_CE), 'train acc:', "{:3.2f}".format(train_acc*100)
          , 'valid CE:', "{:3.2f}".format(valid_CE), 'valid acc:', "{:3.2f}".format(valid_acc*100))

    #Sends the data to tensorboard to track progress.
    tb.on_epoch_end(epoch, {'train_CE': train_CE, 'train_acc': train_acc * 100, 'valid_CE': valid_CE, 'valid_acc': valid_acc * 100})

tb.on_train_end(None)

class_names = list(valid_gen.class_indices)
#Gets the names of the different classes as it will use the model to try and correctly predict what
#the images are.

#Load a validation image.
X_img, X_labels = next(valid_gen)
#Use the model to predict what the image is of.
X_pred = convnet.predict(X_img)
#Evaluate the image.
test_CE, test_acc = convnet.evaluate(X_img, X_labels)
#Prints the accuracy score.
print('testing accuracy:', "{:3.2f}".format(test_acc*100))

#Loops through a selection of images and will try an predict what they are.
for i in range(9):
    plt.subplot(3, 3, i + 1, frameon=False)
    plt.axis('off')
    #Displays the images.
    plt.imshow(np.reshape(X_img[i], input_shape[1:]))
    #Predicts their title, what the model thinks the image is.
    plt.title(class_names[np.argmax(X_pred[i])])
plt.show(block=False)

"""# Feature Map Visualisation"""

#Loads a validation image.
sample_imgs, sample_labels = next(valid_gen)
sess = tf.keras.backend.get_session()
#Loads both the input and output layer of the model.
input_layer = model.layers[0].input
output_layer = model.layers[layer].output
outputs = sess.run(output_layer, feed_dict={input_layer: sample_imgs})

for i in range(8):
    plt.subplot(2, 4, i + 1, frameon=False)
    plt.axis('off')
    plt.imshow(sample_imgs[i]);

feature=7
for i in range(8):
    plt.subplot(2, 4, i + 1, frameon=False)
    plt.axis('off')
    plt.imshow(outputs[i,:,:,feature])

"""# Max Feature of Image"""

#Define the feature.
feature=7
#Define how many batches the model will run through.
n_batches = int(np.ceil(valid_gen.samples / batch_size)) + 1
for batch in range(n_batches):
    print('batch', batch, 'of', n_batches)
    #Loda a validation image.
    sample_imgs, sample_labels = next(valid_gen)
    outputs = sess.run(output_layer, feed_dict={input_layer : sample_imgs})
    feature_response = np.mean(outputs[:,:,:, feature], axis=(1,2))
    if batch == 0:
        top_features = feature_response
        top_images = sample_imgs
    else:
        #Find the top images from the data.
        top_features = np.concatenate((top_features, feature_response))
        top_images = np.concatenate((top_images, sample_imgs))
        
        #Find which feature is the best for image recognition.
        top_idx = np.argsort(top_features)
        top_features = [top_features[i] for i in top_idx[-25:]]
        top_images = [top_images[i] for i in top_idx[-25:]]

#Plot the images that all have similar, top features for recognition.
for i in range(25):
    plt.subplot(5, 5, i + 1, frameon=False)
    plt.axis('off')
    plt.imshow(top_images[i])
plt.show(block=False)

"""# Saliency Mapping"""

import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
from PIL import Image

#Define a method to find the attention map for convolutional layers.
def visualise_conv2d_local(model, image, nth_layer=-2, n_classes=257):
    #Set up a backend session.
    sess = tf.keras.backend.get_session()

    names = [op.name for op in sess.graph.get_operations() if op.type=='Conv2D']

    RegisterGuidedBackprop()
    #Find the activation gradient of the model.
    model = OverrideReluGradient(model, model.input)

    #Gets a prediction about the image.
    y_pred = sess.run(model.output, feed_dict={model.input: image})
    image = np.resize(image, (1,128,128,3))

    #Find the exciting gradients of the image.
    cams = GradCAM(model, y_pred.argmax(), names[nth_layer], sess, image, nb_classes=n_classes)

    return cams, y_pred.argmax()

def RegisterGuidedBackprop():
    tf.RegisterGradient("GuidedReLU")
    def _GuidedReLU(op, grad):
        return grad * tf.cast(grad > 0., op.inputs[0].dtype)

def OverrideReluGradient(model, images):
    with tf.compat.v1.get_default_graph().gradient_override_map({'Relu': 'GuidedReLU'}):
        model.build(images)
    return model

#Method to find the most interesting gradients in the image.
def GradCAM(model, category_index, layer_name, sess, image, nb_classes):
    loss = tf.multiply(model.output, tf.one_hot([category_index], nb_classes))
    reduced_loss = tf.reduce_sum(loss[0])
    conv_output = sess.graph.get_tensor_by_name(layer_name + ':0')
    grads = tf.gradients(reduced_loss, conv_output)[0]
    output, grads_val = sess.run([conv_output, grads], feed_dict={model.input: image})
    weights = np.mean(output, axis=(1, 2))
    cams = np.sum(weights * output, axis=3)
    return cams

#Rescales the image to ensure they're the same dimensions.
def RescaleImage(img, shape, inter):
    x = np.uint8((img - np.min(img)) / (np.max(img) - np.min(img)) * 255)
    x = Image.fromarray(x)
    return x.resize(shape[1:-1], inter)

#Plots the original image.
def PlotImage(img):
    plt.imshow(RescaleImage(img.squeeze(), img.shape, Image.NEAREST), cmap='gray', interpolation='none')

#Plots the image with the saliency heat map.
def PlotCAM(img, cam):
    plt.imshow(RescaleImage(img.squeeze(), img.shape, Image.NEAREST), cmap='gray', interpolation='none')
    plt.imshow(RescaleImage(cam.squeeze(), img.shape, Image.BICUBIC), cmap='jet', interpolation='bicubic', alpha=0.5)

#This is best run using the trained parameter-tuned deep convolutional model
image, label = next(valid_gen)
cam, y_pred = visualise_conv2d_local(model, image, nth_layer=-2, n_classes=257)

#Plot and run the attention mapping.
plt.subplot(1,2,1)
image = np.resize(image, (1, 128, 128, 3))
PlotImage(image)
plt.subplot(1,2,2)
PlotCAM(image, cam)
plt.title(str(y_pred))
plt.show()

"""# Deep Feature Visualisation"""

#This is same model used for the training and testing but has been modified to have deeper layers, this helps with the
#deep feature visualisation but has a worse accuracy score, most likely due to overfitting the data. 

filters = 32
#Filters are used for convolutional layers.
lr = .001
#The learning rate of the model.
epochs = 80
#The epochs are how many times the model will do a full run.
batch_size = 32
decay = lr/epochs
img_size =  128
layer = 6

#Start of the model.
model = tf.keras.Sequential()
#Adding convolutional layers will helped with image recognition.
model.add(tf.keras.layers.Conv2D(filters = (filters),
                                 kernel_size= (3,3),
                                 input_shape = (img_size,img_size,3),
                                 padding = 'same'))
model.add(tf.keras.layers.BatchNormalization())
#Batch normalisation will automatically standarise inputs to the layer. 
model.add(tf.keras.layers.Activation('relu'))
#The relu function is a non-linear function to help with backpropogation. 
model.add(tf.keras.layers.MaxPool2D(pool_size = (2,2)))
#Max pooling can determine the maxmimum features being used by the model. 

#Loop through the important functions a few times to build the model.
for i in range(layer):
    filters1 = filters * (2**(i+1))
    model.add(tf.keras.layers.Conv2D(filters = filters1, kernel_size= (3,3), padding = 'same'))
    model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.Activation('relu'))
    model.add(tf.keras.layers.MaxPool2D(pool_size = (2,2)))

model.add(tf.keras.layers.Dropout(0.2))
#Flatten the dimensions of the tensor object into one dimension. 
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(512))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Activation('relu')) 

#The final layer will give an output.
outputs = model.add(tf.keras.layers.Dense(257,activation='softmax'))
#The model can be compiled using optimisers, the learning rate, a loss functions and accuracy metrics. 
model.compile(optimizer=tf.keras.optimizers.Adam(lr=lr,decay=decay), 
              loss='categorical_crossentropy', 
              metrics=['accuracy'])

#Shows a summary of the model created.
model.summary()

#Sets up a tensorboard to display the training and testing of the model.
tb = tf.keras.callbacks.TensorBoard(log_dir='logs\cnn_transfer7', write_graph=True)
tb.set_model(model)

#The model can be run using the fit_generator and the images will be trained and tested to increase
#the accuracy score. 
model.fit_generator(
        train_gen,
        shuffle=True,
        steps_per_epoch=6000/32,
        epochs=30,
        validation_data=valid_gen,
        validation_steps=1000/32)
#It uses both data generators and shuffles the images before each epoch so data input is always unpredictable.

import matplotlib.pyplot as plt
import numpy as np
import scipy as sc

#Method that uses the convolutional layers to create deep feature visualisations.
def visualise_conv2d_local(model, nth_layer=-1, feature=0, step_size=1, pixel_pct=0, grad_pct=0, L1=0, L2=0, sigma=0, sigma_every=1, epochs=300, verbose=0):
    sess = tf.keras.backend.get_session()

    names = [op.name for op in sess.graph.get_operations() if op.type=='Conv2D']
    layer = sess.graph.get_tensor_by_name(names[nth_layer] + ':0')

    layer_shape = layer.get_shape().as_list()

    activation = layer[:,int(layer_shape[1] / 2),int(layer_shape[2] / 2),feature]
    gradient = tf.gradients(activation, model.input)

    #input_shape = np.squeeze(model.layers[0].output_shape)
    #input_shape[0] = 1

    image = np.zeros(input_shape)
    for i in range(epochs):
        grad, loss_value = sess.run([gradient, activation], feed_dict={model.input: image})
        grad = np.array(grad).squeeze().reshape(input_shape)

        #optimize
        image += step_size * (grad - L2 * 2 * image - L1 * np.sign(image))

        #pixel clipping
        img_norm = np.linalg.norm(image, axis=3)
        img_norm = np.repeat(img_norm, input_shape[-1]).reshape(input_shape)
        pixel_clip = np.percentile(img_norm, pixel_pct)
        image[img_norm <= pixel_clip] = 0

        #gradient clipping
        grad_norm = np.linalg.norm(grad, axis=3)
        grad_norm = np.repeat(grad_norm, input_shape[-1]).reshape(input_shape)
        grad_clip = np.percentile(grad_norm, grad_pct)
        image[grad_norm <= grad_clip] = 0

        #gaussian smoothing
        if i % sigma_every == 0: image = sc.ndimage.gaussian_filter(image, sigma, mode='nearest')

        if verbose == 1 and (i + 1) % int(epochs / 10.0) == 0: print('@epoch', i, 'gradient:', np.linalg.norm(grad))
        if verbose == 2: PlotImage(image, i)

    #remove zero borders
    image = np.delete(image, np.where(~image.any(axis=1))[1], axis=2)
    image = np.delete(image, np.where(~image.any(axis=2))[1], axis=1)

    return image

#Method that rescales the image into a presentable format.
def RescaleImage(img):
    #Rescale the image.
    return (img - np.min(img)) / (np.max(img) - np.min(img))

#Method that plots the image.
def PlotImage(img, i="feature"):
    #Plot the image using the defined feature.
    plt.clf()
    #Rescales before plotting.
    plt.imshow(np.reshape(RescaleImage(img), img.shape[1:]))
    plt.title(str(i))
    plt.pause(0.05)
    plt.show()

#Use sigma functions and verbose to make the image deeper and smoother.
image = visualise_conv2d_local(model, nth_layer=-1, sigma=1, feature=99, verbose=1)
#The image is created by the layers and feature of the deep model.
image = RescaleImage(image)
PlotImage(image)

"""# Semantic Segmentation"""

#Define the batch size of the model.
BATCH_SIZE = 5
N_CLASSES = 32
#Large image dimensions.
img_dims = (256,256)

#Load the training images with image augmentation.
train_img_datagen = tf.keras.preprocessing.image.ImageDataGenerator(
    rescale=1./255., 
    brightness_range=(0.8, 1.2),
    shear_range=10,
    zoom_range=0.2,
    horizontal_flip=True,
    #Validate on a random 10%.
    validation_split=0.1)

#Load the training labels.
train_lbl_datagen = tf.keras.preprocessing.image.ImageDataGenerator(
    rescale=1./8., validation_split=0.1)

#Get the path for the training images.
train_img_gen = train_img_datagen.flow_from_directory(
    'Caltech-256/Caltech-256',
    target_size=img_dims,
    batch_size=BATCH_SIZE,
    class_mode=None,
    color_mode='rgb',
    subset='training',
    seed=1)

train_lbl_gen = train_lbl_datagen.flow_from_directory(
    'Caltech-256/Caltech-256',
    target_size=img_dims,
    batch_size=BATCH_SIZE,
    class_mode=None,
    color_mode='grayscale',
    subset='training',
    seed=1)

#Load the validation data generator.
test_img_datagen = tf.keras.preprocessing.image.ImageDataGenerator(
    rescale=1./255., validation_split=0.1)

test_lbl_datagen = tf.keras.preprocessing.image.ImageDataGenerator(
    rescale=1./8., validation_split=0.1)

#Load the validation images
test_img_gen = test_img_datagen.flow_from_directory(
    'Caltech-256/Caltech-256',
    target_size=img_dims,
    batch_size=BATCH_SIZE,
    class_mode=None,
    color_mode='rgb',
    subset='validation',
    seed=1)

#Load the validation labels.
test_lbl_gen = test_lbl_datagen.flow_from_directory(
    'Caltech-256/Caltech-256',
    target_size=img_dims,
    batch_size=BATCH_SIZE,
    class_mode=None,
    color_mode='grayscale',
    subset='validation',
    seed=1)

#Defines a method for the encoder block for the u-net.
def encoderBlock(inputs, units, activation='linear'):
    net = tf.keras.layers.Conv2D(filters=units,kernel_size=(3,3),activation=activation,padding='same')(inputs)
    net = tf.keras.layers.Conv2D(filters=units,kernel_size=(3,3),activation=activation,padding='same')(net)
    outputs = tf.keras.layers.AveragePooling2D(pool_size=(2,2),strides=(2,2))(net)
    return outputs, net
 
    #Defines a method for the decoder block for the u-net.
def decoderBlock(inputs, units, skip, activation='linear'):
    net = tf.keras.layers.UpSampling2D(size=(2,2))(inputs)
    net = tf.keras.layers.Concatenate()([net, skip])
    net = tf.keras.layers.Conv2D(filters=units,kernel_size=(3,3),activation=activation,padding='same')(net)
    net = tf.keras.layers.Conv2D(filters=units,kernel_size=(3,3),activation=activation,padding='same')(net)
    return net
 
#Recieves the image dimensions as input.
inputs = tf.keras.layers.Input(shape=(img_dims[0],img_dims[1],3))
#Skip layers and encoder being run with the elu activation function.
net, skip1 = encoderBlock(inputs, 64, 'elu')
net, skip2 = encoderBlock(net, 64, 'elu')
net, skip3 = encoderBlock(net, 128, 'elu')
net, skip4 = encoderBlock(net, 256, 'elu')
net, skip5 = encoderBlock(net, 512, 'elu')
#Decoder method is run for the layers.
net = decoderBlock(net, 512, skip5, 'elu')
net = decoderBlock(net, 256, skip4, 'elu')
net = decoderBlock(net, 128, skip3, 'elu')
net = decoderBlock(net, 64, skip2, 'elu')
net = decoderBlock(net, 64, skip1, 'elu')
#Mean is normalised.
net = tf.keras.layers.BatchNormalization()(net)
outputs = tf.keras.layers.Conv2D(filters=N_CLASSES,kernel_size=(1,1),activation='softmax')(net)

#The model is saved and optimised, then the structure of the network is shown.
model = tf.keras.Model(inputs, outputs)
model.compile(optimizer=tf.keras.optimizers.Adam(0.0005, beta_1=0.99),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
model.summary()

import matplotlib.pyplot as plt
import numpy as np

#Plots the prediction of images using the u-net structure.
def plotPrediction(x, y, y_pred):
    plt.subplot(1,3,1)
    plt.imshow(x[0].squeeze())
    plt.subplot(1,3,2)
    plt.imshow(y[0].squeeze())
    plt.subplot(1,3,3)
    y_pred = np.argmax(y_pred, axis=-1)
    plt.imshow(y_pred[0].squeeze())
    plt.show(block=False)

#Connects to tensorboard.
tb = tf.keras.callbacks.TensorBoard(log_dir='logs\semantic_segmentation', histogram_freq=0, batch_size=BATCH_SIZE, write_graph=True, write_grads=False)
tb.set_model(model)

epochs = 20000
for i in range(epochs):
    #Loads in the training and validation images
    x = next(train_img_gen)
    y = next(train_lbl_gen)
    train_eval = model.train_on_batch(x, y)
    
    x = next(test_img_gen)
    y = next(test_lbl_gen)
    test_eval = model.test_on_batch(x, y)
    
    #Saves results to tensorboard.
    tb.on_epoch_end(i, {'train_loss': train_eval[0], 'train acc': train_eval[1]})
    tb.on_epoch_end(i, {'test_loss': test_eval[0], 'test acc': test_eval[1]})
    
    if i % 50 == 0:
        #Shows the predicition.
        y_pred = model.predict(x)
        plotPrediction(x, y, y_pred)
        print('epoch:', i, 'train:', train_eval[0], train_eval[1], 'test:', test_eval[0], test_eval[1], )

tb.on_train_end(None)

